import getpass
import datetime
import inspect
import os

filename = inspect.getframeinfo(inspect.currentframe()).filename
path = os.path.dirname(os.path.abspath(filename))


# user = getpass.getuser()
# date = datetime.datetime.now()
# date = '%i%0.2i%0.2i' % (date.year, date.month, date.day)
# OUTDIR = ''.join((user[0], user[2], date, '_', config["dir_suffix"]))
OUTDIR = ''.join(('cl20161205','_', config["dir_suffix"]))

if 'extract' in config:
    TIMING = glob_wildcards(config["extract"]["timing"])[0]
# TYPE_LIST = ['mapping', 'gDNA', 'cDNA', 'spike']
# TYPE_LIST = [read_type for read_type in TYPE_LIST if read_type in config]


group_name_vec = [group[0] for group in config['groups']]
replicate_dict = {}
if 'replicate' in group_name_vec:
    index = group_name_vec.index('replicate')
    for name in config['input_file']['gDNA'].keys():
        if 'spike' in config['input_file']:
            file_name = '%s/cDNA.%s.cpm.gDNA.spike' % (OUTDIR, name)
        else:
            file_name = '%s/cDNA.%s.cpm.gDNA' % (OUTDIR, name)
        name_split = name.split('_')
        if index < len(name_split):
            name_vec = [name_split[i] for i in range(0, len(name_split))
                        if i != index]
            mean_name = '_'.join(name_vec)
            if mean_name in replicate_dict:
                replicate_dict[mean_name].append(file_name)
            else:
                replicate_dict[mean_name] = [file_name]
rule all:
    input:
        # expand('{outdir}/mapping.{num}.bam', outdir=OUTDIR, num=(1,2)),
        expand('{outdir}/cDNA.{name}.normalized', outdir=OUTDIR, name=config['input_file']['cDNA'].keys())
# rule all:
#     input:
#         expand('{outdir}/bc_{sample}.txt', outdir=OUTDIR,
#                sample=config["intersect"].keys()),
#         expand('{outdir}/bc_timing_{state}.txt', outdir=OUTDIR, state=TIMING),
#         expand('{outdir}/bc_cpg_distance.txt', outdir=OUTDIR)

# rule bedtoolsbed:
#     input:
#         '%s/mapping.stdout.txt' % OUTDIR,
#     params:
#         '%s/mapping.rev_mapping.bed' % OUTDIR
#     output:
#         temp('%s/mapping.rev_mapping2.bed' % OUTDIR)
#     shell:
#         "awk '{{if ($1!=\"*\") print $0}}' {params} | \\"
#         "bedtools sort -i > {output}"
#
# rule bigWigBed:
#     input:
#         '%s/mapping.stdout.txt' % OUTDIR,
#     params:
#         '%s/mapping.rev_mapping.bed' % OUTDIR
#     output:
#         temp('%s/mapping.rev_mapping3.bed' % OUTDIR)
#     run:
#         command = "awk '{{if ($1!=\"*\") print $1\"\t\"$2\"\t\"$3\"\t\"$4\"_\"$5\"/\"$6}}' {params} > {output}"
#         shell(command)
# #
# # for SAMPLE in config['extract']:
# rule:
#     input:
#         map='%s/mapping.rev_mapping3.bed' % OUTDIR,
#         lst=config["extract"]['timing']
#     output:
#         '%s/bc_timing_{state}.txt' % OUTDIR
#     shell:
#         '%s/scripts/extract.sh {input.map} < {input.lst} > {output}' % (path)
#
# rule nearest:
#     input:
#         map='%s/mapping.rev_mapping2.bed' % OUTDIR,
#         lst=config["nearest"]['cpg']
#     output:
#         '%s/bc_cpg_distance.txt' % OUTDIR
#     shell:
#         '%s/scripts/nearest.sh {input.map} < {input.lst} > {output}' % (path)
#
#
#
#
# for SAMPLE in config['intersect'].keys():
#     TRACK=config["intersect"][SAMPLE]
#     if '{outdir}' in TRACK:
#         TRACK = expand(TRACK, outdir=OUTDIR)
#     rule:
#         input:
#             map='%s/mapping.rev_mapping2.bed' % OUTDIR,
#             track=TRACK
#         params:
#             SAMPLE
#         output:
#             '%s/bc_%s.txt' % (OUTDIR, SAMPLE)
#         shell:
#             '%s/scripts/intersect.sh {input.map} {params} < {input.lst} > {output}' % (path)
#
#
#
# for READ_TYPE in config["file_list"]:
#     rule:
#         input:
#             lst=config["file_list"][READ_TYPE],
#             cfg=config["config"]
#         output:
#             dir='/'.join((OUTDIR, READ_TYPE)),
#             stdout='%s/%s/stdout.txt'%(OUTDIR, READ_TYPE),
#             bed=expand('{outdir}/mapping.rev_mapping.bed', outdir=OUTDIR)
#         threads: 10
#         shell:
#             "mkdir -p {output.dir};"
#             "~/python/bin/python src/python/trip.py -t {threads} -o {output.dir} -l {input.lst} -c {input.cfg} -u -v -d 2>&1 | tee {output.stdout}"
#
# rule format_rep:
#     input: config["repeatMasker"]
#     output: expand('{outdir}/repeats.bed', outdir=OUTDIR)
#     run:
#         command = ("awk -F'[|\\t]' '{{if(NR==1){{print \"barcode\\tclass\\tfamily\\tname\\tcount\\ttotal\"}}"
#                    "else {{\n"
#                    "  if ($2 ~/\//){{\n"
#                    "    match($2,/(.*)\/(.*)/, a)\n"
#                    "    class=a[1]\n"
#                    "    fam=$2\n"
#                    "  }} else {{\n"
#                    "    class=$2\n"
#                    "    fam=$2\"/-\"\n"
#                    "  }}"
#                    "  print $1\"\\t\"class\"\\t\"fam\"\\t\"$3\"\\t\"$4\"\\t\"$5\n"
#                    "}}}}' < {input} > {output}")
#                 #    "mv %s/bc_repeat.tmp %s/bc_repeat.txt")%(OUTDIR, OUTDIR, OUTDIR, OUTDIR)
#         shell(command)
# rule trip:
#   output:
#     dir=expand("{outdir}/{type}/", outdir=OUTDIR, type=config["file_list"])
#     stdout=expand("{outdir}/{type}/stdout.txt", outdir=OUTDIR, type=config["file_list"])
#   input:
#      lst=config["file_list"]["norm_exp"]
#      cfg=config["config"]
#   shell:
#     "mkdir -p {output.dir}"
#     "nice -19 ~/python/bin/python src/python/trip.py -t {THREADS} -o {output.dir} -l {input.lst} -c {input.cfg} -u -v -d 2>&1 | tee {output.stdout}"


# rule parse_sam:
#     input:
#         bam=expand('{outdir}/mapping.{{num}}.bam', outdir=OUTDIR),
#         count=expand('{outdir}/mapping.{{map}}.starcode.count', outdir=OUTDIR)
#     output:
#         '{outdir}/mapping.{num}.bed',
#         '{outdir}/mapping.{num}.table'
#     run:
#         starcode_set = set()
#         for count_file in input.count:
#             with open(count_file) as cf:
#                 for line in cf.readlines():
#                     barcode = line.split('\t')[0]
#                     if barcode



if 'mapping' in config['input_file']:
    rule align:
        input:
            expand('{outdir}/mapping.{name}.{{num}}.fastq.gz', outdir=OUTDIR,
                   name=config['input_file']['mapping'].keys())
        output:
            '{outdir}/mapping.{num}.bam'
        params:
            config['bowtie'],
            '{num}'
        threads: 10
        log:
            '{outdir}/mapping.align.{num}.log'
        run:
            options = ' '.join(params[0]['options'][params[1]])
            index = params[0]['index']
            gunzip = "gunzip -c {input}"
            ## filter for read length
            awk = ("awk '{{"
                   "       step=NR%4;"
                   "       if (step==0 && length(a[2])>6){{"
                   "           for (i in a){{"
                   "               print a[i]"
                   "           }}"
                   "           print $0"
                   "       }} else if (step!=0){{"
                   "           a[step]=$0;"
                   "       }}"
                   "}}'")
            bowtie = 'bowtie2 -p {threads} %s -x %s -U - > {output}' % (options,
                                                                        index)
            flagstat = 'samtools flagstat {output} > {log}'
            print(bowtie)
            shell('%s | %s | %s; %s' % (gunzip, awk, bowtie, flagstat))

###############################################################################
##+++++++++++++++++++++++++++++ mean expression +++++++++++++++++++++++++++++##
###############################################################################


# rule mean_exp:
#     input:
#         lambda wildcards: replicate_dict[wildcards.mean_name]
#     output:
#         '%s/cDNA.{mean_name}.mean',
#         '%s/cDNA.{mean_name}.mean.cut'
#     run:
#         cpm_dict = {}
#         mean_file = open('{output[0]}', 'w')
#         mean_cut_file = open('{output[1]}', 'w')
#         for input_file in snakemake.input:
#             with open(input_file) as file_in:
#                 for line in file_in.readlines():
#                     norm_cpm, barcode = line.strip().split()
#                     if barcode in cpm_dict:
#                         cpm_dict[barcode][input_file] = float(norm_cpm)
#                     else:
#                         cpm_dict[barcode] = {input_file: float(norm_cpm)}
#         for barcode in cpm_dict:
#             if len(cpm_dict[barcode]) == len(snakemake.input):
#                 mean = sum(cpm_dict[barcode].values())/len(snakemake.input)
#                 mean_file.write('%f\t%s' % (mean, barcode))
#         mean_file.close()
#         mean_cut_file.close()




###############################################################################
##++++++++++++++++++++++ calculate counts per million +++++++++++++++++++++++##
###############################################################################
#
# rule cpm:
#     input:
#         expand('{outdir}/{read_type}.{{name}}.starcode.count', outdir=OUTDIR,
#                read_type = ('cDNA', 'gDNA', 'spike'))
#     output:
#         '{outdir}/{read_type}.{name}.cpm'
#     shell:
#         "awk '{{arr[$2] = $1; sum += $1}}"
#         "END{{for (bc in arr){{print arr[bc]/sum*1000000\"\t\"bc}}}}'"
#         "< {input} > {output}"

if 'spike' in config['input_file']:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA.{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA.{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/spike.{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA.{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'
else:
    rule normalize_mean_expression:
        input:
            expand('{outdir}/cDNA.{{name}}.starcode.count', outdir=OUTDIR),
            expand('{outdir}/gDNA.{{name}}.starcode.count', outdir=OUTDIR)
        output:
            '{outdir}/cDNA.{name}.normalized'
        params:
            path
        shell:
            'Rscript {params}/scripts/normalize.R {input} {output}'

###############################################################################
##++++++++++++++++++++++++ select genuine barcodes ++++++++++++++++++++++++++##
###############################################################################

rule starcode_cDNA:
    input:
        expand('{outdir}/cDNA.{{name}}.raw.count', outdir=OUTDIR),
        '{outdir}/gDNA.{name}.starcode.count'
    output:
        gen='{outdir}/cDNA.{name}.starcode.count',
        mut='{outdir}/cDNA.{name}.genuine.cut',
        notg='{outdir}/cDNA.{name}.in_gDNA.cut',
        notc='{outdir}/gDNA.{name}.in_cDNA.cut',
        count='{outdir}/cDNA.{name}.count.cut'
    params:
        lev_dist= config['lev_dist'],
        use_other= True,
        count= config['min_count']['cDNA']
    threads:
        3
    script:
        'scripts/starcode.py'

rule starcode_gDNA:
    input:
        expand('{outdir}/gDNA.{{name}}.raw.count', outdir=OUTDIR)
    output:
        gen='{outdir}/gDNA.{name}.starcode.count',
        mut='{outdir}/gDNA.{name}.genuine.cut',
        count='{outdir}/gDNA.{name}.count.cut'
    params:
        lev_dist= config['lev_dist'],
        use_other= False,
        count= config['min_count']['gDNA']
    threads:
        3
    script:
        'scripts/starcode.py'


rule starcode_spike_pool:
    input:
        expand('{outdir}/spike_pool.raw.count', outdir=OUTDIR)
    output:
        gen='{outdir}/spike_pool.starcode.count',
        mut='{outdir}/spike_pool.genuine.cut',
        count='{outdir}/spike_pool.count.cut'
    params:
        lev_dist= config['lev_dist'],
        use_other= False,
        count= config['min_count']['spike']
    threads:
        3
    script:
        'scripts/starcode.py'


rule starcode_spike_sample:
    input:
        expand('{outdir}/spike.{{name}}.raw.count', outdir=OUTDIR)
    output:
        gen='{outdir}/spike.{name}.starcode.count',
        mut='{outdir}/spike.{name}.genuine.cut',
        count='{outdir}/spike.{name}.count.cut'
    params:
        lev_dist= config['lev_dist'],
        use_other= False,
        count= 0
    threads:
        3
    script:
        'scripts/starcode.py'

rule starcode_map:
    input:
        expand('{outdir}/mapping.raw.count', outdir=OUTDIR)
    output:
        gen='{outdir}/mapping.starcode.count',
        mut='{outdir}/mapping.genuine.cut',
        count='{outdir}/mapping.count.cut'
    params:
        lev_dist= config['lev_dist'],
        use_other= False,
        count= config['min_count']['map']
    threads:
        3
    script:
        'scripts/starcode.py'

if 'mapping' in config['input_file']:
    rule combine_starcode_map:
        input:
            expand('{outdir}/mapping.{map}.raw.count', outdir=OUTDIR,
                   map=config['input_file']['mapping'].keys())
        output:
            '{outdir}/mapping.raw.count'
        shell:
            'cat {input} > {output}'


rule count_barcode:
    input:
        '%s/{file_base}.barcode.txt.gz' % OUTDIR
    output:
        '%s/{file_base}.raw.count' % OUTDIR
    shell:
        "gunzip -cf - < {input} | awk '{{print $3}}' | tail -n+2 | sort | uniq -c | awk '{{print $2\"\t\"$1}}'> {output}"

###############################################################################
##+++++++++++++++++++++++++++++++ parse reads +++++++++++++++++++++++++++++++##
###############################################################################

if 'gDNA' in config['input_file']:
    rule parse_gDNA:
        input:
            lambda wildcards: config['input_file']['gDNA'][wildcards.name][0]
        output:
            '%s/gDNA.{name}.barcode.txt.gz' % (OUTDIR),
            '%s/gDNA.{name}.statistics.txt' % (OUTDIR),
            structure = '%s/gDNA.{name}.structure.txt' % (OUTDIR)
        log:
            '%s/gDNA.{name}_parser.log' % (OUTDIR)
        params:
            structure= config['structure']['gDNA'],
            type_dict= config['input_file']['gDNA'],
            outdir = OUTDIR
        run:
            structure = params.structure % params.type_dict[wildcards.name][1]
            if params.type_dict[wildcards.name][1] == 0:
                structure = re.sub('index.*\n', '', structure)
            with open(output.structure, 'w') as f:
                f.write(structure)
            shell('~t.v.schaik/modules/read-parsing/read_parser.py -r -l {log} '
                  '-b gDNA.{wildcards.name} {input} {output.structure} {params.outdir}')

if 'cDNA' in config['input_file']:
    rule parse_cDNA:
        input:
            lambda wildcards: config['input_file']['cDNA'][wildcards.name][0]
        output:
            '%s/cDNA.{name}.barcode.txt.gz' % (OUTDIR),
            '%s/cDNA.{name}.statistics.txt' % (OUTDIR),
            structure = '%s/cDNA.{name}.structure.txt' % (OUTDIR)
        log:
            '%s/cDNA.{name}_parser.log' % (OUTDIR)
        params:
            structure= config['structure']['cDNA'],
            type_dict= config['input_file']['cDNA'],
            outdir = OUTDIR
        run:
            structure = params.structure % params.type_dict[wildcards.name][1]
            if params.type_dict[wildcards.name][1] == 0:
                structure = re.sub('index.*\n', '', structure)
            with open(output.structure, 'w') as f:
                f.write(structure)
            print(input)
            shell('~t.v.schaik/modules/read-parsing/read_parser.py -r -l {log} '
                  '-b cDNA.{wildcards.name} {input} {output.structure} {params.outdir}')


if 'spike' in config['input_file']:
    THIS_BASE = '%s/spike' % OUTDIR
    if not os.path.exists(THIS_BASE):
        os.makedirs(THIS_BASE)
    rule parse_spike_pool:
        input:
            config['input_file']['spike'][0]
        output:
            '%s_pool.barcode.txt.gz' % (THIS_BASE),
            '%s_pool.statistics.txt' % (THIS_BASE),
            structure = '%s_pool.structure.txt' % (THIS_BASE)
        log:
            '%s.pool_parser.log' % (THIS_BASE)
        params:
            structure = config['structure']['spike'],
            index_len = config['input_file']['spike'][1],
            name = 'pool',
            outdir = OUTDIR
        run:
            structure = params.structure % params.index_len
            if params.index_len == 0:
                structure = re.sub('index.*\n', '', structure)
            with open(output.structure, 'w') as f:
                f.write(structure)
            shell('~t.v.schaik/modules/read-parsing/read_parser.py -r -l {log} '
                  '-b spike_{params.name} {input} %s {params.outdir}' % output.structure)

    rule parse_spike_sample:
        input:
            lambda wildcards: config['input_file']['cDNA'][wildcards.name][0]
        output:
            '%s.{name}.barcode.txt.gz' % (THIS_BASE),
            '%s.{name}.statistics.txt' % (THIS_BASE),
            structure = '%s.{name}.structure.txt' % (THIS_BASE)
        log:
            '%s.{name}_parser.log' % (THIS_BASE)
        params:
            structure= config['structure']['spike'],
            type_dict= config['input_file']['cDNA'],
            outdir = OUTDIR
        run:
            structure = params.structure % params.type_dict[wildcards.name][1]
            if params.type_dict[wildcards.name][1] == 0:
                structure = re.sub('index.*\n', '', structure)
            with open(output.structure, 'w') as f:
                f.write(structure)
            shell('~t.v.schaik/modules/read-parsing/read_parser.py -r -l {log} '
                  '-b spike.{wildcards.name} {input} %s {params.outdir}' % output.structure)


if 'mapping' in config['input_file']:

    rule parse_mapping:
        input:
            lambda wildcards: config['input_file']['mapping'][wildcards.name][0]
        output:
            '%s/mapping.{name}.barcode.txt.gz' % (OUTDIR),
            '%s/mapping.{name}.1.fastq.gz' % (OUTDIR),
            '%s/mapping.{name}.2.fastq.gz' % (OUTDIR),
            '%s/mapping.{name}.statistics.txt' % (OUTDIR),
            structure = '%s/mapping.{name}.structure.txt' % (OUTDIR)
        log:
            '%s/mapping.{name}_parser.log' % (OUTDIR)
        params:
            structure= config['structure']['mapping'],
            type_dict= config['input_file']['mapping'],
            outdir = OUTDIR,
            name= '{name}'
        run:
            structure = params.structure % params.type_dict[wildcards.name][1]
            structure = structure.replace('\\', '')
            if params.type_dict[wildcards.name][1] == 0:
                structure = re.sub('index.*\n', '', structure)
            with open(output.structure, 'w') as f:
                f.write(structure)
            shell('~t.v.schaik/modules/read-parsing/read_parser.py -r -l {log} -p {input[1]} '
                  '-b mapping.{wildcards.name} {input[0]} {output.structure} {params.outdir}')
